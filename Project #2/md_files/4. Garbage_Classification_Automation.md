# 2022-1_DTA_Garbage_Classification

​	

# Garbage Classification Automation

2022.06.17. Fri

​	

Handong Global University, School of Mechanical and Control Engineering, 2022-1 DigitalTwin&Automation

21500018 | 강희윤

21700791 | 홍세현

21900726 | 지현빈

​	

​	

## Environment

### Download
재활용품 분류 및 로봇 자동화를 실시간으로 구현한 프로그램은 Anaconda의 가상환경과 Visual Code를 활용하여 구동시킨다. 프로그램 구동을 위한 파일은 아래의 이미지와 같이 python 파일 2개와 3개의 폴더가 필요하다. 

Download link: [Garbage_Classification_Automation](https://github.com/Hongsehyun/2022_1_DigitalTwin_Automation/tree/main/Project%20%232/3.%20Garbage_Classification_Automation), [Pre-Trained_Data_and_Model](https://github.com/Hongsehyun/2022_1_DigitalTwin_Automation/blob/main/Project%20%232/Pre-trained%20Data%20and%20Model.zip)

Pre-Trained_Data_and_Model.zip은 압축을 풀면 아래 이미지의 data 폴더와 같다. 

![image](https://user-images.githubusercontent.com/91526930/174454647-1e83ab7d-b105-424c-9f76-90a0ba0de831.png)

### Anaconda Virtual Environment

* Anaconda Installation Guide: [here](https://ykkim.gitbook.io/dlip/installation-guide/anaconda)

* Anaconda 설치를 완료하면, Anaconda Prompt를 관리자 권한으로 실행한다. 그리고, Anaconda를 업데이트한다. 
<pre><code>conda update -n base -c defaults conda</code></pre>

* python version 3.9의 가상환경을 생성한다.
<pre><code>conda create -n py39_dta python=3.9</code></pre>

* activate를 통해 생성된 가상환경으로 들어간다. 
<pre><code>conda activate py39_dta</code></pre>

* 가상환경 내부에 다음 패키지들을 설치한다. 이 패키지들은 프로그램 구동을 위해 사용되는 것들이다. 
<pre><code>pip install numpy
pip install opencv-python
pip install fastai==1.0.61
pip install tensorflow</code></pre>

​	

​

## Real-Time Image Processing
![image](https://user-images.githubusercontent.com/91526930/174454402-ae332e39-60d6-4117-a160-e315babf480b.png)

'Garbage_Classification.py' 파일에서는 실시간으로 물체를 인식하고, 분류하는 과정을 거친다. 이후 분류된 쓰레기에 대한 정보를 텍스트 파일에 저장함으로 Robot_Operation.py로 정보를 넘겨준다. 

* **Object Detection with Motion**

컨베이어 벨트 위에 놓인 물체는 한 방향으로 움직이고 있는 상황이다. 카메라 영상에서 연속적인 움직임으로 인식되는 것을 물체로 판단하고 있으며, 물체가 특정 구간에 진입 시 이미지를 캡처하도록 하였다. 특정 구간은 이미지 상에 x축 방향으로 400번째 pixel을 넘어선 순간으로 정하였다. 인식된 물체가 특정 구간에 진입시 flag = 1을 할당함으로써 이미지 분류 작업으로 진행된다.
```python
# 플로 결과 그리기 + 물체 Tracking하며 중심좌표 출력하기
def drawFlow(img, flow, flag_change_ok, step=10):                   # step = 그리드 한 칸 한 칸의 간격 지정
    h, w = img.shape[:2]
    flag = 0
    x_center = 0
    y_center = 0
    
    # 10픽셀 간격의 그리드 인덱스 구하기
    idx_y, idx_x = np.mgrid[step/2 : h : step , step/2 : w : step].astype(np.int) 
    indices = np.stack((idx_x, idx_y), axis=-1).reshape(-1, 2)      # 각 포인트의 좌표를 모두 생성
    
    
    # 인덱스 순회
    for x, y in indices:
        # 각 그리드 인덱스 위치에 점 그리기
        cv.circle(img, (x, y), 1, (0, 212, 255), 1)                 # img, center, radius, color, thickness

        # 각 그리드 인덱스에 해당하는 플로 결과 값(이동 거리)
        dx, dy = flow[y, x].astype(np.int)
        
        # Thres Val [1] - 충분히 이동하였는가 :: 노이즈, 먼지 등을 탐지하는 경우를 1차로 방지!
        if dx > 5 or dx < -5 or dy > 5 or dy < -5:           
            x_idx.append(x)                                
            y_idx.append(y)
            
            x_center = sum(x_idx)/len(x_idx)                                    
            y_center = sum(y_idx)/len(y_idx)
            
        # 각 그리드 인덱스 위치에서 이동한 거리만큼 선 그리기
        cv.line(img, (x, y), (x+dx, y+dy), (0, 212, 255), 2, cv.LINE_AA)     # img, center, radius, color, thickness
    
    
    # Thres Val [2] - 물체가 실존하는가 :: 노이즈, 먼지 등을 탐지하는 경우를 2차로 방지!
    
    if len(x_idx)>15 and len(y_idx)>15 and x_center > 400:                                     
        if flag_change_ok == 1:
            # print('x_center: {},   y_center: {}\n'.format(x_center, y_center))
            flag = 1
    
    return flag, x_center, y_center
```


* **Getting and Resizing Image**

물체가 인식된 후에, 이미지를 사전에 준비된 폴더에 저장한다. 저장된 이미지는 사전에 훈련된 모델에 적용시키기 위한 이미지 사이즈로 불러온다. 
```python
# When object Detected
if flag == 1:
    # Folder Clearing
    cv.putText(frame_dst, 'Detected!', (210, 55), cv.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)

    print(DeleteAllFiles('data/test_real'))
    print(DeleteAllFiles('RealTime_Test_Input_Image'))

    # Get Cam Image and Resizing
    cv.imwrite("RealTime_Test_Input_Image/test_image.jpg",frame_src)
    src = cv.imread('RealTime_Test_Input_Image/test_image.jpg')
    dst = cv.resize(src, dsize=(512, 384), interpolation=cv.INTER_AREA)
    cv.imwrite('data/test_real/test_image.jpg', dst)

    flag = 2
```


* **Garbage Classification**

불러온 이미지를 훈련된 모델에 적용하여, 쓰레기의 종류를 예측한다. 쓰레기의 종류는 'cardboard', 'metal', 'plastic' 이다. 분류된 쓰레기에 대한 정보를 'garbage_type.txt'파일에 저장된다. 이 정보를 Robot_Operation.py에서 읽어들인다. 
```python
# Predict
if flag == 2:
    # Test Data Define
    data = ImageDataBunch.from_folder(path, test="test_real")

    # Model Define (Get Pre-Trained Model)
    learn = cnn_learner(data, models.resnet34, metrics=error_rate)
    learn.load(model_file_name)

    img = learn.data.test_ds[0][0]

    # Predict Test Data
    preds = learn.predict(img)

    max_idxs = np.argmax(preds[2])
    max_idxs = np.array(max_idxs)
    max_idxs_li.append(max_idxs)

    # 부여된 번호를 토대로 class의 이름을 부여
    yhat.append(data.classes[max_idxs])

    # 출력
    # print("predict = {}\n\n".format(yhat[0]))
    cv.putText(frame_dst, yhat[0], (410, 55), cv.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)
    cv.putText(frame_src, yhat[0], (410, 55), cv.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)

    f = open('garbage_type.txt', 'w')
    f.write(yhat[0])
    f.close()

    for i in range(5):
        print(yhat[0])

    flag = 0
    time.sleep(1)
```
​	

​

## Robot Automation with INDY-10
![image](https://user-images.githubusercontent.com/91526930/174454410-a46534c9-2139-4589-a42e-f86aa69f241e.png)

'Robot_Operation.py'에서는 'garbage_type.txt'파일에 적혀있는 재활용품 종류를 파악하여, 로봇을 구동함으로 분리수거를 진행한다.

* **Initialization**

PC와 로봇을 연결하고, 로봇의 움직임과 관련한 셋팅 변수들(충돌, 관절 움직임 속도 등)을 선언한다. 
``` python
robot_ip = "192.168.0.6"    # Robot (Indy) IP
robot_name = "NRMK-Indy10"  # Robot name (Indy7)indy

# Create class object
indy = client.IndyDCPClient(robot_ip, robot_name)

indy.connect()

indy.set_collision_level(5)
indy.set_joint_vel_level(7)
indy.set_task_vel_level(7)
indy.set_joint_blend_radius(20)
indy.set_task_blend_radius(0.2)

j_pos = indy.get_joint_pos()
t_pos = indy.get_task_pos()
```

* **GO_HOME & WAIT**

로봇은 처음 연결시, Home 위치로 이동한다. Home 위치는 컨베이어 벨트 위 재활용품을 잡으러 가기 전의 위치이다. Home 위치에 도달 후, 'robot_state.txt' 파일에 현재 로봇이 움직이고 있지 않다는 의미의 '0'이라는 flag를 저장한다. 곧바로, 로봇은 대기 모드로 전환되며, 계속해서 'garbage_type.txt'파일에 쓰인 정보를 읽는다. 만약 읽을 텍스트가 존재한다면, 재활용품이 인식되고 분류된 정보를 넘겨받은 것을 의미한다. 이어서, 로봇은 읽어들인 재활용품 정보를 저장한 후, 'garbage_type.txt'를 초기화하고, 'robot_state.txt'에 로봇이 움직인다는 의미의 '1'이라는 flag를 저장한다.

``` python
if mode == GO_HOME:

    indy.task_move_to(home_pos)

    while True:
        status = indy.get_robot_status()
        sleep(0.2)
        if status[key[5]]==1 :
            break

    f_robot = open("robot_state.txt", "w")
    f_robot.write("0")
    f_robot.close()
    mode = WAIT

elif mode == WAIT:

    f_garb = open('garbage_type.txt', 'r')
    garb_type = f_garb.readline()
    f_garb.close()

    if garb_type != '':

        f_garb = open('garbage_type.txt', 'w')
        f_garb.write('')
        f_garb.close()

        f_robot = open("robot_state.txt", "w")
        f_robot.write("1")
        f_robot.close()

        mode = GO_GARBAGE_GRIP
```

​	

​

## Garbage Classification Automation with INDY-10
![image](https://user-images.githubusercontent.com/91526930/174454418-27e3e239-7e89-487f-8817-e7bc9a605eda.png)

​	

​	
